<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BPE Tokenizer</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
        }

        .container {
            max-width: 1000px;
            margin: 0 auto;
            background: white;
            border-radius: 15px;
            box-shadow: 0 20px 40px rgba(0,0,0,0.1);
            overflow: hidden;
        }

        .header {
            background: linear-gradient(135deg, #2c3e50 0%, #34495e 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }

        .header h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
        }

        .header p {
            font-size: 1.1em;
            opacity: 0.9;
        }

        .main-content {
            padding: 30px;
        }

        .section {
            margin-bottom: 30px;
            padding: 25px;
            border-radius: 10px;
            background: #f8f9fa;
            border-left: 4px solid #667eea;
        }

        .section h2 {
            color: #2c3e50;
            margin-bottom: 15px;
            font-size: 1.5em;
        }

        .info-box {
            background: #e8f4fd;
            border: 1px solid #bee5eb;
            color: #0c5460;
            padding: 15px;
            border-radius: 8px;
            margin-bottom: 20px;
        }

        .input-group {
            margin-bottom: 20px;
        }

        .input-group label {
            display: block;
            margin-bottom: 8px;
            font-weight: 600;
            color: #34495e;
        }

        .input-group textarea,
        .input-group input {
            width: 100%;
            padding: 12px;
            border: 2px solid #e0e0e0;
            border-radius: 8px;
            font-size: 14px;
            transition: border-color 0.3s;
        }

        .input-group textarea:focus,
        .input-group input:focus {
            outline: none;
            border-color: #667eea;
        }

        .input-group textarea {
            resize: vertical;
            min-height: 100px;
        }

        .btn {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border: none;
            padding: 12px 25px;
            border-radius: 8px;
            cursor: pointer;
            font-size: 16px;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            margin-right: 10px;
            margin-bottom: 10px;
        }

        .btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 5px 15px rgba(102, 126, 234, 0.4);
        }

        .btn.secondary {
            background: linear-gradient(135deg, #95a5a6 0%, #7f8c8d 100%);
        }

        .output {
            background: #2c3e50;
            color: #ecf0f1;
            padding: 20px;
            border-radius: 8px;
            font-family: 'Courier New', monospace;
            white-space: pre-wrap;
            max-height: 300px;
            overflow-y: auto;
            margin-top: 15px;
            display: none;
        }

        .stats-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin-top: 20px;
        }

        .stat-card {
            background: white;
            padding: 20px;
            border-radius: 10px;
            text-align: center;
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
        }

        .stat-card .number {
            font-size: 2em;
            font-weight: bold;
            color: #667eea;
            margin-bottom: 5px;
        }

        .stat-card .label {
            color: #7f8c8d;
            font-size: 0.9em;
        }

        .token-display {
            display: flex;
            flex-wrap: wrap;
            gap: 5px;
            margin-top: 15px;
        }

        .token {
            background: #3498db;
            color: white;
            padding: 4px 8px;
            border-radius: 4px;
            font-size: 12px;
            font-family: monospace;
        }

        .special-token {
            background: #e74c3c;
        }

        .success {
            background: #d4edda;
            border: 1px solid #c3e6cb;
            color: #155724;
            padding: 15px;
            border-radius: 8px;
            margin-top: 15px;
        }

        .error {
            background: #f8d7da;
            border: 1px solid #f5c6cb;
            color: #721c24;
            padding: 15px;
            border-radius: 8px;
            margin-top: 15px;
        }

        .loading {
            text-align: center;
            padding: 20px;
            color: #667eea;
        }

        .spinner {
            border: 3px solid #f3f3f3;
            border-top: 3px solid #667eea;
            border-radius: 50%;
            width: 30px;
            height: 30px;
            animation: spin 1s linear infinite;
            margin: 0 auto 10px;
        }

        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ü§ñ BPE Tokenizer</h1>
            <p>Train on text to learn vocabulary, then encode/decode efficiently</p>
        </div>

        <div class="main-content">
            <!-- Info Section -->
            <div class="info-box">
                <strong>üß† What is Training?</strong> The tokenizer learns vocabulary by finding frequent character pairs in your text and merging them into tokens. For example: "h" + "e" ‚Üí "he", "l" + "l" ‚Üí "ll". More text = better vocabulary!
            </div>

            <!-- Training Section -->
            <div class="section">
                <h2>üìö Train Tokenizer</h2>
                <div class="input-group">
                    <label for="trainingText">Training Text (the text to learn from):</label>
                    <textarea id="trainingText" placeholder="Enter your training text here...">The quick brown fox jumps over the lazy dog. Machine learning is a subset of artificial intelligence. Natural language processing helps computers understand human language. Tokenization is the process of breaking text into smaller units called tokens. Byte pair encoding is an efficient subword tokenization algorithm.</textarea>
                </div>
                <div class="input-group">
                    <label for="vocabSize">Vocabulary Size:</label>
                    <input type="number" id="vocabSize" value="200" min="50" max="2000">
                </div>
                <button class="btn" onclick="trainTokenizer()">üîß Train Tokenizer</button>
                <button class="btn secondary" onclick="loadSample()">üìÑ Load Sample</button>
                
                <div id="trainingLoading" class="loading" style="display: none;">
                    <div class="spinner"></div>
                    <p>Training tokenizer...</p>
                </div>
                <div id="trainingOutput" class="output"></div>
            </div>

            <!-- Statistics Section -->
            <div class="section">
                <h2>üìä Statistics</h2>
                <div class="stats-grid">
                    <div class="stat-card">
                        <div class="number" id="totalVocab">4</div>
                        <div class="label">Total Vocabulary</div>
                    </div>
                    <div class="stat-card">
                        <div class="number" id="specialTokens">4</div>
                        <div class="label">Special Tokens</div>
                    </div>
                    <div class="stat-card">
                        <div class="number" id="mergeRules">0</div>
                        <div class="label">Merge Rules</div>
                    </div>
                    <div class="stat-card">
                        <div class="number" id="regularTokens">0</div>
                        <div class="label">Regular Tokens</div>
                    </div>
                </div>
            </div>

            <!-- Encoding Section -->
            <div class="section">
                <h2>üî§ Encode Text</h2>
                <div class="input-group">
                    <label for="encodeText">Text to Encode:</label>
                    <textarea id="encodeText" placeholder="Enter text to encode...">Hello world! This is a test.</textarea>
                </div>
                <button class="btn" onclick="encodeText()">‚û°Ô∏è Encode</button>
                <button class="btn secondary" onclick="clearEncode()">üóëÔ∏è Clear</button>
                
                <div id="encodeOutput" class="output"></div>
                <div id="tokenDisplay" class="token-display"></div>
            </div>

            <!-- Decoding Section -->
            <div class="section">
                <h2>üî¢ Decode Tokens</h2>
                <div class="input-group">
                    <label for="decodeTokens">Token IDs (comma-separated):</label>
                    <input type="text" id="decodeTokens" placeholder="2,15,23,45,3">
                </div>
                <button class="btn" onclick="decodeTokens()">‚¨ÖÔ∏è Decode</button>
                <button class="btn secondary" onclick="clearDecode()">üóëÔ∏è Clear</button>
                
                <div id="decodeOutput" class="output"></div>
            </div>
        </div>
    </div>

    <script src="tokenizer.js"></script>
    <script>
        // Global tokenizer instance
        let tokenizer = new BPETokenizer();

        // Sample data
        const sampleText = `
The quick brown fox jumps over the lazy dog. Machine learning is a subset of artificial intelligence that focuses on algorithms and statistical models. Natural language processing is a subfield of linguistics, computer science, and artificial intelligence.

Tokenization is the process of breaking text into words, phrases, symbols, or other meaningful elements called tokens. The list of tokens becomes input for further processing such as parsing or text mining.

Byte pair encoding is a simple data compression technique that iteratively replaces the most frequent pair of bytes in a sequence with a single, unused byte. It was first described by Philip Gage in a February 1994 article.

Deep learning is part of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.

Transformers are a type of neural network architecture that has been state-of-the-art on many natural language processing tasks. The key innovation is the self-attention mechanism, which allows the model to weigh the importance of different words.

Pre-trained language models are neural language models trained on a large corpus of text data. These models learn to predict the next word in a sequence, which requires them to develop an understanding of language syntax, semantics, and world knowledge.
        `.trim();

        function loadSample() {
            document.getElementById('trainingText').value = sampleText;
            document.getElementById('vocabSize').value = 300;
        }

        async function trainTokenizer() {
            const text = document.getElementById('trainingText').value;
            const vocabSize = parseInt(document.getElementById('vocabSize').value);
            
            if (!text.trim()) {
                showMessage('trainingOutput', 'Please enter training text', 'error');
                return;
            }

            // Show loading
            document.getElementById('trainingLoading').style.display = 'block';
            document.getElementById('trainingOutput').style.display = 'none';

            try {
                tokenizer = new BPETokenizer();
                
                let output = `üîß Training tokenizer...\nCorpus length: ${text.length} characters\nTarget vocabulary: ${vocabSize} tokens\n\n`;
                
                // Simulate training progress
                await new Promise(resolve => {
                    setTimeout(() => {
                        output += 'üìä Analyzing character frequencies...\n';
                        updateOutput(output);
                        
                        setTimeout(() => {
                            output += 'üîç Finding frequent character pairs...\n';
                            updateOutput(output);
                            
                            setTimeout(() => {
                                output += 'üîó Learning merge rules...\n';
                                updateOutput(output);
                                
                                // Actually train the tokenizer
                                tokenizer.train(text, vocabSize, false);
                                
                                const stats = tokenizer.getVocabStats();
                                output += `\n‚úÖ Training complete!\n`;
                                output += `üìä Final vocabulary: ${stats.totalVocabSize} tokens\n`;
                                output += `üîó Merge rules learned: ${stats.mergeRules}\n`;
                                output += `‚ö° Ready to encode/decode text!`;
                                
                                updateOutput(output);
                                updateStats();
                                
                                // Hide loading
                                document.getElementById('trainingLoading').style.display = 'none';
                                
                                showMessage('trainingOutput', 'Tokenizer trained successfully!', 'success');
                                resolve();
                            }, 800);
                        }, 600);
                    }, 400);
                });

            } catch (error) {
                document.getElementById('trainingLoading').style.display = 'none';
                showMessage('trainingOutput', `Training failed: ${error.message}`, 'error');
            }
        }

        function updateOutput(text) {
            const output = document.getElementById('trainingOutput');
            output.textContent = text;
            output.style.display = 'block';
        }

        function updateStats() {
            const stats = tokenizer.getVocabStats();
            document.getElementById('totalVocab').textContent = stats.totalVocabSize;
            document.getElementById('specialTokens').textContent = stats.specialTokens;
            document.getElementById('mergeRules').textContent = stats.mergeRules;
            document.getElementById('regularTokens').textContent = stats.regularTokens;
        }

        function encodeText() {
            const text = document.getElementById('encodeText').value;
            
            if (!text.trim()) {
                showMessage('encodeOutput', 'Please enter text to encode', 'error');
                return;
            }

            try {
                const startTime = performance.now();
                const tokenIds = tokenizer.encode(text);
                const endTime = performance.now();
                
                let output = `üìù Input: "${text}"\n`;
                output += `üî¢ Encoded: [${tokenIds.join(', ')}]\n`;
                output += `üìè Token count: ${tokenIds.length}\n`;
                output += `‚ö° Time: ${(endTime - startTime).toFixed(2)}ms`;
                
                document.getElementById('encodeOutput').textContent = output;
                document.getElementById('encodeOutput').style.display = 'block';
                
                // Show tokens visually
                displayTokens(tokenIds);
                
                // Auto-fill decode input
                document.getElementById('decodeTokens').value = tokenIds.join(', ');
                
            } catch (error) {
                showMessage('encodeOutput', `Encoding failed: ${error.message}`, 'error');
            }
        }

        function displayTokens(tokenIds) {
            const container = document.getElementById('tokenDisplay');
            container.innerHTML = '';
            
            // Get token details
            const reverseVocab = new Map();
            for (const [token, id] of tokenizer.vocab) {
                reverseVocab.set(id, token);
            }
            
            tokenIds.forEach(id => {
                const token = reverseVocab.get(id);
                const tokenElement = document.createElement('span');
                tokenElement.className = tokenizer.specialTokens.has(token) ? 'token special-token' : 'token';
                tokenElement.textContent = `${token} (${id})`;
                tokenElement.title = `Token: ${token}, ID: ${id}`;
                container.appendChild(tokenElement);
            });
        }

        function decodeTokens() {
            const tokensInput = document.getElementById('decodeTokens').value;
            
            if (!tokensInput.trim()) {
                showMessage('decodeOutput', 'Please enter token IDs', 'error');
                return;
            }

            try {
                const tokenIds = tokensInput.split(',').map(id => parseInt(id.trim())).filter(id => !isNaN(id));
                
                if (tokenIds.length === 0) {
                    showMessage('decodeOutput', 'Please enter valid token IDs', 'error');
                    return;
                }
                
                const startTime = performance.now();
                const decodedText = tokenizer.decode(tokenIds);
                const endTime = performance.now();
                
                let output = `üî¢ Input: [${tokenIds.join(', ')}]\n`;
                output += `üìù Decoded: "${decodedText}"\n`;
                output += `‚ö° Time: ${(endTime - startTime).toFixed(2)}ms`;
                
                document.getElementById('decodeOutput').textContent = output;
                document.getElementById('decodeOutput').style.display = 'block';
                
            } catch (error) {
                showMessage('decodeOutput', `Decoding failed: ${error.message}`, 'error');
            }
        }

        function clearEncode() {
            document.getElementById('encodeOutput').style.display = 'none';
            document.getElementById('tokenDisplay').innerHTML = '';
            document.getElementById('encodeText').value = '';
        }

        function clearDecode() {
            document.getElementById('decodeOutput').style.display = 'none';
            document.getElementById('decodeTokens').value = '';
        }

        function showMessage(elementId, message, type) {
            const element = document.getElementById(elementId);
            const messageDiv = document.createElement('div');
            messageDiv.className = type;
            messageDiv.textContent = (type === 'error' ? '‚ùå ' : '‚úÖ ') + message;
            element.appendChild(messageDiv);
            element.style.display = 'block';
        }

        // Initialize
        document.addEventListener('DOMContentLoaded', function() {
            updateStats();
        });
    </script>
</body>
</html>